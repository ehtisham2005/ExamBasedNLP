Here is a comprehensive summary file. Save this as **`PROJECT_SUMMARY.md`**.

This document contains every technical detail, architectural decision, and file description needed for anyone (including "Future You") to pick up this project instantly.

---

# ðŸ“˜ Project State: AI Exam Guiding System

**Date:** January 15, 2026
**Status:** Backend Logic & Visualization Complete (Production Grade)

## 1. Project Overview

An intelligent study assistant that parses unstructured exam data (Syllabus, Previous Year Questions) to generate study plans. It uses modern NLP (Transformers) to understand the *meaning* of topics rather than just keywords, fetches high-quality study material from the web, and maps cognitive dependencies between topics.

## 2. Tech Stack & Libraries

* **Language:** Python 3.x
* **Core AI Model:** `sentence-transformers` (Model: `all-MiniLM-L6-v2`)
* *Why?* Lightweight, fast, and optimized for semantic search and clustering.


* **Search Engine:** `google-api-python-client` (Google Custom Search JSON API)
* *Why?* Replaced unstable scrapers with the official API for guaranteed high-quality tutorial results.


* **Content Extraction:** `trafilatura`
* *Why?* Extracts main study text from HTML while stripping ads, sidebars, and menus.


* **Graph Visualization:** `pyvis` & `networkx`
* *Why?* Generates interactive, physics-based HTML knowledge graphs with community detection.


* **Metrics:** `textstat`
* *Why?* Calculates "Time to Study" using Flesch-Kincaid reading ease scores + word count.


* **Security:** `python-dotenv`
* *Why?* Manages API keys securely.



## 3. Key Algorithms Implemented

1. **Semantic Importance Ranking (SBERT + Cosine Similarity):**
* Compares Syllabus Topics vectors against PYQ vectors.
* Identifies "High Yield" topics even if phrasing differs (e.g., matching "CPU Scheduling" to "Process Execution").


2. **Deep Content Relation Mapping:**
* Fetches full textual content for every topic from the internet.
* Embeds the *entire* article content.
* Calculates an All-vs-All correlation matrix.
* **Logic:** If Topic A and Topic B have >35% semantic overlap in their *content*, they are linked as "Related/Prerequisite."


3. **Community Detection (Clustering):**
* Uses `networkx.community.greedy_modularity_communities`.
* Automatically groups topics into cognitive clusters (e.g., "Design Phase", "Testing Phase", "Management") based on connection density.


4. **Smart Caching System:**
* Hashes topic names (MD5) to create unique filenames.
* Stores downloaded content in `cache_content/`.
* Prevents redundant API calls and speeds up analysis from 3 minutes to <1 second.



## 4. File Structure & Functionality

```text
ExamBasedNLP/
â”‚
â”œâ”€â”€ .env                     # [SECURE] Stores GOOGLE_API_KEY and SEARCH_ENGINE_ID
â”œâ”€â”€ .gitignore               # Ignores .env, cache, and venv
â”œâ”€â”€ requirements.txt         # List of dependencies
â”œâ”€â”€ knowledge_graph.html     # [OUTPUT] Interactive visualization generated by visualizer.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ syllabus.txt         # Raw input: Line-separated topics
â”‚   â””â”€â”€ pyqs.txt             # Raw input: Previous Year Questions
â”‚
â”œâ”€â”€ cache_content/           # [AUTO] Stores downloaded study materials (.txt files)
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ analyzer.py          # Core logic: Ranks topics by importance vs PYQs
    â”œâ”€â”€ fetcher.py           # The "Internet Agent". Handles Google API, cleaning, and caching.
    â”œâ”€â”€ deep_relations.py    # The "Brain". Fetches content -> Embeds -> Finds connections.
    â”œâ”€â”€ visualizer.py        # The "Face". Runs relations analysis -> Builds Clustered Graph HTML.
    â””â”€â”€ loader.py            # Utility: Safe text file loading helper.

```

## 5. Current Capabilities

* **âœ… Robust Data Fetching:** Successfully queries Google API, filters for "tutorials," and extracts clean text.
* **âœ… Intelligent Mapping:** Automatically discovers relationships (e.g., linking "Software Maintenance" to "Evolution").
* **âœ… Interactive Visualization:** Generates a `knowledge_graph.html` where topics are colored by cluster and linked by similarity strength.
* **âœ… Resilience:** Handles API failures, empty search results, and "stale cache" issues gracefully.

## 6. How to Run (Transition Guide)

**Step 1: Setup Keys**
Ensure `.env` exists in root:

```ini
GOOGLE_API_KEY=your_key_here
SEARCH_ENGINE_ID=your_cx_id_here

```

**Step 2: Generate the Knowledge Graph**
This is the main demo script currently.

```bash
.venv/Scripts/activate
streamlit run src/app.py
```

* *First Run:* Takes ~2 mins (downloads data via API).
* *Subsequent Runs:* Instant (loads from `cache_content/`).
* *Output:* Opens `knowledge_graph.html`.

**Step 3: Analyze Importance (PYQs)**
To see which topics are most important for exams:

```bash
python src/analyzer.py --pyqs data/pyqs.txt --syllabus data/syllabus.txt

```

## 7. Immediate Next Steps

1. **Frontend Integration:** Build a `Streamlit` app (`src/app.py`) to replace the command line interface.
2. **User Inputs:** Allow users to upload PDF Syllabus/PYQs instead of editing `.txt` files manually.
3. **Study Plan Generation:** Use the metrics (time-to-study) to output a schedule (e.g., "Day 1: Study the Red Cluster").